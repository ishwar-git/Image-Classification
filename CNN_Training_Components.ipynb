{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50200607",
   "metadata": {},
   "source": [
    "# CNN Training Components\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ishwar-git/Image-Classification/blob/main/CNN_Training_Components.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d9efd9",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This notebook captures the **content outline** for training a Convolutional Neural Network (CNN).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9be5a1",
   "metadata": {},
   "source": [
    "## CNN Training Components\n",
    "\n",
    "1. **Dataset Preparation**\n",
    "   - Collect and preprocess data (resize, normalize, augment)\n",
    "   - Split into train, validation, and test sets\n",
    "\n",
    "2. **Model Initialization**\n",
    "   - Define CNN architecture (Conv, ReLU, Pool, Dense)\n",
    "   - Initialize weights (Xavier, He, random normal)\n",
    "\n",
    "3. **Forward Propagation**\n",
    "   - Input image passes through CNN layers\n",
    "   - Convolution → Activation → Pooling → Flatten → Fully Connected → Output\n",
    "   - Compute predicted output (ŷ)\n",
    "\n",
    "4. **Loss Function**\n",
    "   - Measures difference between predicted (ŷ) and true (y)\n",
    "   - Common choices:\n",
    "     - **Cross-Entropy Loss** (classification)\n",
    "     - **Mean Squared Error (MSE)** (regression)\n",
    "\n",
    "5. **Backward Propagation (Backprop through Convolution)**\n",
    "   - Compute gradients of loss w.r.t. weights\n",
    "   - Chain Rule propagates error from output → input layers\n",
    "   - Update gradients using optimizer\n",
    "\n",
    "6. **Optimizer**\n",
    "   - Adjusts weights to minimize loss  \n",
    "   - Examples:\n",
    "     - **SGD (with Momentum)**\n",
    "     - **Adam**\n",
    "     - **RMSProp**\n",
    "   - Update rule:  \n",
    "\n",
    "     \\[ w_{new} = w_{old} - \\eta \\times \\frac{\\partial L}{\\partial w} \\]\n",
    "\n",
    "7. **Regularization Techniques**\n",
    "   - **Dropout**: randomly deactivate neurons to avoid overfitting  \n",
    "   - **Weight Decay (L2 Regularization)**  \n",
    "   - **Batch Normalization**\n",
    "\n",
    "8. **Hyperparameter Tuning**\n",
    "   - Learning rate, batch size, number of epochs, optimizer choice\n",
    "   - Grid search / random search / Bayesian optimization\n",
    "\n",
    "9. **Evaluation & Validation**\n",
    "   - Use validation/test set to measure performance  \n",
    "   - Metrics:\n",
    "     - Accuracy, Precision, Recall, F1-score  \n",
    "     - Confusion matrix  \n",
    "     - ROC curve\n",
    "\n",
    "10. **Visualization & Monitoring**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861d5816",
   "metadata": {},
   "source": [
    "### Flow Summary\n",
    "```\n",
    "Dataset → CNN Model → Forward Pass → Loss Calculation → Backpropagation \n",
    "→ Optimizer Update → Regularization → Evaluation → Visualization\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "278086f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install torch torchvision torchaudio  # (Uncomment if needed)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f257f744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(50000, 10000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) Dataset Preparation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Example with CIFAR10 (auto-downloads if online; otherwise, replace with your dataset)\n",
    "train_ds = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "test_ds  = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=2)\n",
    "test_loader  = DataLoader(test_ds, batch_size=128, shuffle=False, num_workers=2)\n",
    "len(train_ds), len(test_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5e44cd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SmallCNN(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=16384, out_features=128, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.3, inplace=False)\n",
       "    (4): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2) Model Initialization (simple CNN example)\n",
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 16 * 16, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "model = SmallCNN().to(device)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3da1d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Loss Function & Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)  # L2 = weight_decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebccf8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Training Loop (Forward → Loss → Backprop → Optimizer Step)\n",
    "def train_one_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss, total_correct, total = 0.0, 0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        # Forward\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Metrics\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        total_correct += (preds == y).sum().item()\n",
    "        total += x.size(0)\n",
    "    return total_loss / total, total_correct / total\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss, total_correct, total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            total_correct += (preds == y).sum().item()\n",
    "            total += x.size(0)\n",
    "    return total_loss / total, total_correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5c055ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 | train_loss=1.4563 acc=0.476 | val_loss=1.1257 acc=0.600\n",
      "Epoch 2/2 | train_loss=1.1326 acc=0.600 | val_loss=1.0173 acc=0.637\n"
     ]
    }
   ],
   "source": [
    "# 5) Fit (demo: small number of epochs)\n",
    "EPOCHS = 2\n",
    "for epoch in range(EPOCHS):\n",
    "    tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "    te_loss, te_acc = evaluate(model, test_loader, criterion)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | train_loss={tr_loss:.4f} acc={tr_acc:.3f} | val_loss={te_loss:.4f} acc={te_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5865e041",
   "metadata": {},
   "source": [
    "> **Note:** If this environment doesn't have internet access, CIFAR-10 won't auto-download. Replace the dataset block with your local data loader or run this notebook in your own environment where downloads are allowed.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
